from flask import Flask, request, jsonify
from flask_cors import CORS
import fitz  # PyMuPDF
import pdfplumber
import os
from openai import OpenAI

app = Flask(__name__)
CORS(app)

client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# VariÃ¡veis globais temporÃ¡rias (memÃ³ria do Ãºltimo embarque)
ultimo_embarque = None
ultimo_temp_text = ""
ultimo_sm_text = ""

@app.route('/')
def home():
    return 'Coldchain backend estÃ¡ no ar! ðŸš€'

@app.route('/analisar', methods=['POST'])
def analisar():
    global ultimo_embarque, ultimo_temp_text, ultimo_sm_text

    embarque = request.form.get('embarque')
    temp_pdf = request.files.get('temps')
    sm_pdf = request.files.get('sm')

    # Simular extraÃ§Ã£o de temperaturas para o grÃ¡fico
# Em produÃ§Ã£o, extraia isso do PDF
temperaturas = [6, 7, 8.5, 9, 7, 5, 3, 1.5, 2, 6]  # Exemplo
labels = [f"{2*i} min" for i in range(len(temperaturas))]

# Separar cores por valor
datasets = [
    {
        "label": "Temperatura",
        "data": temperaturas,
        "borderColor": ["red" if t > 8 or t < 2 else "green" for t in temperaturas],
        "backgroundColor": "transparent",
        "pointBackgroundColor": ["red" if t > 8 or t < 2 else "green" for t in temperaturas],
        "tension": 0.4,
    },
    {
        "label": "Limite MÃ¡x (8Â°C)",
        "data": [8]*len(temperaturas),
        "borderColor": "rgba(255,0,0,0.3)",
        "borderDash": [5, 5],
        "pointRadius": 0,
        "fill": False
    },
    {
        "label": "Limite MÃ­n (2Â°C)",
        "data": [2]*len(temperaturas),
        "borderColor": "rgba(0,0,255,0.3)",
        "borderDash": [5, 5],
        "pointRadius": 0,
        "fill": False
    }
]

grafico = {
    "tipo": "line",
    "labels": labels,
    "datasets": datasets
}

# Incluir no retorno
return jsonify({'report_md': resultado.strip(), 'grafico': grafico})

    if not embarque or not temp_pdf or not sm_pdf:
        return jsonify({'error': 'Faltam dados no formulÃ¡rio'}), 400

    try:
        # Leitura do relatÃ³rio de temperatura
        temp_text = ''
        with fitz.open(stream=temp_pdf.read(), filetype="pdf") as doc:
            for page in doc:
                temp_text += page.get_text()

        # Leitura do SM
        sm_text = ''
        sm_pdf.stream.seek(0)
        with pdfplumber.open(sm_pdf.stream) as pdf:
            for page in pdf.pages:
                sm_text += page.extract_text() or ''

        # Armazena na memÃ³ria para uso posterior no chat
        ultimo_embarque = embarque
        ultimo_temp_text = temp_text[:3000]  # limitar tamanho para o prompt
        ultimo_sm_text = sm_text[:3000]

        # Prompt para cabeÃ§alho e anÃ¡lise executiva
        prompt = f"""
VocÃª Ã© um analista tÃ©cnico de cadeia fria. Gere um relatÃ³rio executivo para o embarque abaixo, com os seguintes tÃ³picos:
1. CabeÃ§alho com Nome do Cliente, Origem e Destino (data/hora), se presentes no conteÃºdo.
2. Breve resumo tÃ©cnico da excursÃ£o de temperatura, se houver desvios.
3. Pontos crÃ­ticos encontrados.
4. Se possÃ­vel, sugerir melhorias preventivas.

RELATÃ“RIO DE TEMPERATURA:
{ultimo_temp_text}

RELATÃ“RIO SM:
{ultimo_sm_text}
"""

        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "VocÃª Ã© um analista tÃ©cnico em transporte refrigerado."},
                {"role": "user", "content": prompt}
            ]
        )

        gpt_response = response.choices[0].message.content.strip()

        # SimulaÃ§Ã£o de grÃ¡fico de temperatura
        grafico_data = {
            "tipo": "line",
            "labels": ["00h", "06h", "12h", "18h"],
            "datasets": [{
                "label": "Temperatura (Â°C)",
                "data": [2.5, 3.0, 4.1, 3.3],
                "borderColor": "#007bff",
                "fill": False
            }]
        }

        return jsonify({
            'report_md': gpt_response,
            'grafico': grafico_data
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/chat', methods=['POST'])
def chat():
    global ultimo_embarque, ultimo_temp_text, ultimo_sm_text

    data = request.get_json()
    pergunta = data.get("pergunta")

    if not pergunta:
        return jsonify({"erro": "Pergunta nÃ£o enviada."}), 400

    if not ultimo_embarque:
        return jsonify({"erro": "Nenhum embarque foi analisado ainda."}), 400

    try:
        contexto = f"""
VocÃª estÃ¡ ajudando com o embarque: {ultimo_embarque}.
Use os dados abaixo para responder com precisÃ£o:

RELATÃ“RIO DE TEMPERATURA:
{ultimo_temp_text}

RELATÃ“RIO SM:
{ultimo_sm_text}
"""

        resposta = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "VocÃª Ã© um especialista tÃ©cnico em cadeia fria e transporte refrigerado."},
                {"role": "user", "content": contexto},
                {"role": "user", "content": pergunta}
            ]
        )
        mensagem = resposta.choices[0].message.content.strip()
        return jsonify({"resposta": mensagem})
    except Exception as e:
        return jsonify({"erro": str(e)}), 500

if __name__ == '__main__':
    port = int(os.environ.get("PORT", 5000))
    app.run(host="0.0.0.0", port=port)
